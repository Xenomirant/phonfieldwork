---
title: "Phonetic fieldwork or experiments with `phonfieldwork`"
author: "G. Moroz"
bibliography: bibliography.bib
output: 
  html_document:
    smooth_scroll: false
    number_sections: true
    self_contained: yes
    df_print: paged
editor_options: 
  chunk_output_type: console
---

# Introduction

There are a lot of different books about fieldwork and experimenting (e. g. @gordon03, @bowern15). This tutorial covers only data organization part, so I still recommend to read them. In this tutorial I will focuse on such cases when fieldworker or experimenter clearly knows what she or he wants to analyse and already created a list of stimuli that she or he wants to record. For know `phonfieldwork` works only with `.wav` files, but I have a plans to extend its functionality to other types of data (e. g. Sign language video). In the following sections I will discribe my workflow of phonetic fieldwork or experimenting.

# Install the package

Before you start make sure, that you installed a package. This could be done with the following command:

```{r, eval = FALSE}
install.packages("phonfieldwork")
```

This command will install the last stable version of `phonfieldwork` package from CRAN. Since CRAN run multiple package checks before make it available, this variant the most safe way. It is also possible to download the development version from GitHub:

```{r, eval = FALSE}
install.packages("devtools")
devtools::install_github("agricolamz/phonfieldwork")
```

If you have any trouble installing the package, you won't be able to use its functionality. In that case you can write [an issue on Github](https://github.com/agricolamz/phonfieldwork/issues) or an email. Since this package could completely destroy your data, **please don't use it until you are sure that you have made a backup**.

It is impossible to use a package without importanting it. Use the `library()` command for loading the package:
```{r}
library("phonfieldwork")
```

# Philosophy of the `phonfieldwork` package

There are following steps in most phonetic research:

1. Formulate a research question. Think in advance what kind of data will answer to this question, what apropriate amount of data will you need, what kind of annotation will you make, what kind of visualisations and statistical models will you use etc.
2. Create a list of stimuli.
3. Elicite a list of stimuli from speakers who signed an *Informed Consent* statement agreeing to participate in the experiment and record it on audio or/and video.
4. Annotate gathered data.
5. Extract annotated data.
6. Create visualisations and evaluate your statistical models.
7. Report your results.
8. Publish your data.

The `phonfieldwork` package is created for helping with items 3., partially with 4., 5. and 8.

In order to make it easear to automatically annotate gathered data, usualy I collect each stimulus in a separate audiofile.  While recording, I carefully listen to my informants to make sure they are giving the kind of speech I want: three isolated repetitions separated by a pause and within a carrier phrase. Since I'm not afraid to ask my informants to repeat anything that is not done correctly, so as a result of my fieldwork session I will get: 

* a collection of small soundfiles;
* a list of correct and wrong attempts to produce a stimulus (usually I do it in a regular notebook with a regular pen).

There are some phoneticians who will insist on the total recording, since you need to document everything, particularly during the fieldwork. I think that this is actually splitting your tasks: you canâ€™t have your cake and eat it too. But even if you want to have a recording of everything, it is possible to run two recorders: one could work during the whole time of the phonetic session, and another could be used to produce small audio files. There are some phoneticians who will use some software for automatic recording your stimuli (e. g. ...) on a computer.

It is possible to show stimulus by stimulus to a native speaker and ask her or him to pronounce them or **not** to show stimuli to speaker and elicite them just pronouncing stimuli or its translation. In order to collect all stimuli in the particular order and without any missings I use presentations.

Since each stimuli recoreded in a separate audiofile, it is possible automatically merge them into one file and make a stimuli annotation in Praat TextGrid (the same result could be achieved by `Concatenate recoverably` command in Praat). After this step user need to do some annotation by her/his own (for now, the `phonfieldwork` package only works with Praat TextGrids). When the annotation part is finished, it is possible to extract annotated parts to some table, where each annotated object is a row characterised by some features (stimulus, repetition, speaker etc...) could be played and could be seen through oscilogramm and spectrogram. Here is [an example of such a database](https://agricolamz.github.io/from_sound_to_html_viewer/create_html.html) and [instruction for doing it](https://github.com/agricolamz/from_sound_to_html_viewer).

# The `phonfieldwork` package in use
## Make a list of your stimuli

There are several ways of providing infromation about a stumuli list into R:

* using `c()` function you could create a **vector** of all words and store it in a variable `my_stimuli` (you can choose any other name):

```{r}
my_stimuli <- c("tip", "tap", "top")
```

* it is also possible to store your list as a column in a `.csv` file and read it to R using `read.csv()` function:

```{r}
my_stimuli_df <- read.csv("my_stimuli_df.csv")
my_stimuli_df
```

* it is also possible to store your list as a column in an `.xls` or `xlsx` file and read it to R using `read_xls` or `read_xlsx` functions from the package `readxl`. If the package `readxl` is not installed on your computer install it using `install.packages("readxl")`

```{r}
library("readxl")
# run install.packages("readxl") in case you don't have it installed
my_stimuli_df <- read_xlsx("my_stimuli_df.xlsx")
my_stimuli_df
```

## Create a presentation based on a list of stimuli

When the stimuli list is loaded to R, it is possible to create a presentation for elicitation. It is important to define an output directory, so in the following example I use `getwd()` function that returns a path to the current working directory (you coud set any directory as a current one using `setwd()` function), but it is possible to provide a whole path (e. g. "/home/user_name/...").

```{r}
create_presentation(stimuli = my_stimuli_df$stimuli,
                    output_file = "first_example",
                    output_dir = getwd()) 
```

As a result in an output folder were created a file "first_example.html" (you can change this name chaning `output_file` argument) that looks like the following:

<iframe src="https://agricolamz.github.io/phonfieldwork/first_example.html" width = 900 height = 700>
  <p>Your browser does not support iframes :(</p>
</iframe>

It is possible to change an output format in the `output_format` argument: by dafault it is "html", but you can use "pptx" (it is a new feature of `rmarkdown`, so update the package in case you get errors).

## Rename collected data
After collecting data and removing wrong soundfiles one could end up with the following structure:

```{bash, echo = FALSE}
tree | tail -n 10 | head -n 8
```

For each speaker `s1` and `s2` there is a folder that containes three audiofiles. Lets rename the files.

```{r}
rename_soundfiles(stimuli = my_stimuli_df$stimuli,
                  prefix = "s1_",
                  path = "s1/")
```

As a result one can obtain the following structure:

```{bash, echo = FALSE}
tree | tail -n 14 | head -n 12
```

The `rename_soundfiles()` function created a backup folder with all unrenamed files and renamed all files using `prefix` from `prefix` argument. There is an additional argument `backup` that could be set to `FALSE` (it is `TRUE` by default), in case you are sure that renaming function works properly with your files and stimuli.

```{r}
rename_soundfiles(stimuli = my_stimuli_df$stimuli,
                  prefix = "s2_",
                  suffix = paste0("_", 1:3),
                  path = "s2/",
                  backup = FALSE)
```

```{bash, echo = FALSE}
tree | tail -n 14 | head -n 12
```

The last command renamed soundfiles in `s2` folder adding prefix `s2` as in previous example, and adding suffix `1`-`3`. On most operational systems it is impossible to create two files with the same name, so sometimes it could be useful to add some kind of index at the end of the files.

For now `phonfieldwork` can work only with `.wav` files, but I will try to implement more possibilities in the future (such as ELAN .eaf and EXMARaLDA ...).

## Merge all data together

After all files are renamed, it is possible to merge them into one:

```{r}
concatenate_soundfiles(file_name = "s1_all",
                       path = "s1/")
```

This comand creates a new soundfile `s1_all.wav` and an asociated Praat TextGrid `s1_all.TextGrid`:

```{bash, echo = FALSE}
tree | tail -n 16 | head -n 14
```

The result file could be parsed with Praat:

![](images/01_concatenate_soundfiles_result.png)

## Annotate your data

It is possible to create an annotation of words using existing annotation:

```{r}
my_stimuli_df$stimuli
annotate_textgrid(annotation = my_stimuli_df$stimuli,
                  textgrid = "s1/s1_all.TextGrid")
```

![](images/02_annotate_textgrid_result.png)

As it could be seen from the example the `annotate_textgrid()` function creates a backup of the tier, and add a new tier on top of the previous one. It is possible to prevent this function of doing so, by setting a `backup` argument to `FALSE`.

It is possible to annotate every second (third, fourth etc.) interval. Imagine that somebody annotated each vowel in the recording, so the TextGrid will looks like the following:

```{r, include=FALSE}
annotation <- read.csv("annotation_of_s1.csv")
annotation$annotation <- ""
df_to_tier(df = annotation, textgrid = "s1/s1_all.TextGrid") 
```

![](images/03_annotate_textgrid_result.png)

So it is possible to use the second column in `my_stimuli_df` that contain vowels:

```{r}
my_stimuli_df$vowel
annotate_textgrid(annotation = my_stimuli_df$vowel,
                  textgrid = "s1/s1_all.TextGrid",
                  each = 2,
                  tier = 3, 
                  backup = FALSE)
```

![](images/04_annotate_textgrid_result.png)
From the last figure one can see that no backup tier was created (`backup = FALSE`), exactly the third tier was annotated (`tier = 3`), and annotation were performed in every second interval (`each = 2`).

## Extracting your data

It is possible extract all annotated files according some annotation tier:

```{r}
extract_intervals(file_name = "s1/s1_all.wav", 
                  textgrid = "s1/s1_all.TextGrid",
                  tier = 3,
                  path = "s1")
```

```{bash, echo = FALSE}
tree | tail -n 19 | head -n 17
```


## Visualising your data
It is possible to visualise an oscilogram and spetrogram of any sound file:

```{r, fig.width=12, fig.height=6}
draw_sound(file_name = "s1/s1_tip.wav")
```
  
There are additional parametres:

* `title` -- the title for the plot.
* `colores` -- for color spectrogram (`TRUE`), for greyscale (`FALSE`). It is also possible to provide a vector of colors for creating the spectrogram.
* `maximum_frequency` -- the maximum frequency to be displayed for the spectrogram.
* `dynamic_range` -- values greater than this many dB below the maximum will be displayed in the same color.
* `window_length` -- the desired analysis window length in milliseconds.
* `output_file` -- the name of the output file
* `output_width` -- the width of the device
* `output_height` -- the height of the device
* `output_units` -- the units in which height and width are given. Can be "px" (pixels, the default), "in" (inches), "cm" or "mm".

If the `output_file` argument is provided, then instead of creating plot in R it will be saved on the users disk:

```{r}
draw_sound(file_name = "s1/s1_tap.wav", output_file = "s1/s1_tap")
```

```{bash, echo = FALSE}
tree | tail -n 20 | head -n 18
```

It is also possible to create pictures from all .wav files in a folder. For this purposes one need to specify a folder with sound files (argument `sounds_from_folder`) and folder name for pictures (argument `pic_folder_name`). The new picture folder alwayes created in the folder of the upper level, so sound and picture folders are on the same level in the folder tree structure:

```{r}
draw_sound(sounds_from_folder = "s2", pic_folder_name = "s2_pics")
```

```{bash, echo = FALSE}
tree | tail -n 24 | head -n 22
```
```{r, include=FALSE}
# return file structure to the previous one
file.remove(c(paste0("s1/backup/", list.files("s1/backup/")),
              "s1/backup/",
              "s1/s1_all.wav",
              "s1/s1_all.TextGrid",
              "s1/s1_tap.png",
              "s1/Ã¦-2.wav",
              "s1/É’-3.wav",
              "s1/Ä±-1.wav",
              paste0("s2_pics/", list.files("s2_pics/")),
              "s2_pics/"))
rename_soundfiles(stimuli = c("02", "01", "03"),
                  path = "s2/",
                  backup = FALSE)
rename_soundfiles(stimuli = c("02", "01", "03"),
                  path = "s1/",
                  backup = FALSE)
```

# References
